{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>SiteWatch is a command-line tool that allows for the monitoring of Islandora websites using tests specified in a CSV (or similar) file. The tool can be run manually through the command line or can be scheduled to run automatically using a cron job. Additionally, SiteWatch can be configured to send email notifications to multiple emails when a monitored website is down or when certain test cases fail. </p>"},{"location":"#features","title":"Features","text":"<ul> <li>Monitor Islandora websites using tests specified in a CSV file</li> <li>Run tests manually or automatically using a cron job</li> <li>Send email notifications to multiple emails when a monitored website is down or when certain test cases fail</li> <li>Intuitive, easy-to-use command-line interface</li> <li>Open source and free to use</li> </ul>"},{"location":"#usage","title":"Usage","text":"<p>Within the <code>site_watch</code> directory, run the following command, providing the name of your configuration file (\"config.yml\" in this example):</p> <p><code>./site_watch config.yml</code></p> <p>Note</p> <p>If you're on Windows, you will likely need to run SiteWatch by explicitly invoking Python, e.g. <code>./site_watch config.yml</code> instead of using <code>./sitewatch</code> as illustrated above.</p> <p>If your configuration file is not in the same directory as the <code>sitewatch</code> script, use its absolute path, e.g.:</p> <p><code>./site_watch /path/to/config.yml</code></p> <p>SiteWatch will run every test in the input data file referenced in the configuration file, displaying a progress bar and the results of each test as it runs.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions to this documentation are welcome. If you have a suggestion, please open an issue on the SiteWatch issues page and tag your issue \"documentation\".</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#the-setup-wizard","title":"The Setup Wizard","text":"<p>Instead of creating your own configuration file, you can use the setup wizard to create one for you. To run the setup wizard, run the following command: <pre><code>$ ./setupwizard\n</code></pre> The setup wizard will ask you a series of questions, and then create a configuration file for you. You can then edit the configuration file to add any additional settings you need.</p>"},{"location":"configuration/#the-configuration-file","title":"The Configuration File","text":"<p>SiteWatch gets its first input from a configuration file whose path is passed to it as a command line argument. This file is a YAML file, and can be named anything you like, but it must have a <code>.yml</code> extension. For example, if your configuration file is named <code>my_config.yml</code>, you would run SiteWatch like this: <pre><code>$ ./sitewatch my_config.yml\n</code></pre> The configuration file contains all the information SiteWatch needs to run. It contains the location of the CSV-like file containing the tests that SiteWatch is to run, the email addresses of the people who are to receive the test results, and other information. For example, one of the the simplist configuration files might look like this: <pre><code>csv: /path/to/test/csv\n</code></pre></p>"},{"location":"configuration/#required-configuration-settings","title":"Required Configuration Settings","text":""},{"location":"configuration/#input-source","title":"Input Source","text":"<p>Exactly one of the following settings must be present in the configuration file to tell SiteWatch where to find the tests to run:</p> <ul> <li><code>csv</code>: A path to a CSV file containing the tests to run.</li> <li><code>excel</code>: A path to an Excel file containing the tests to run.</li> <li><code>google_sheets</code>: A URL to a Google Sheets file containing the tests to run.</li> </ul>"},{"location":"configuration/#optional-configuration-settings","title":"Optional Configuration Settings","text":""},{"location":"configuration/#email-settings","title":"Email Settings","text":"<p>SiteWatch can send email notifications to one or more recipients when it finishes running. To enable this feature, the configuration file must include the following <pre><code>email:\nsender_name: &lt;The name to appear on the emails&gt;\nsender_email: &lt;The email address to send from; must be an email this machine can send from&gt;\nrecipient_emails: - &lt;email address 1&gt;\n- &lt;email address 2&gt;\n</code></pre></p>"},{"location":"configuration/#minimum-age-of-output-files-to-delete","title":"Minimum Age of Output Files to Delete","text":"<p>SiteWatch produces output <code>.log</code> and <code>.csv</code> files (see Output) for each test it runs. These files are stored in the <code>output</code> directory. By default, SiteWatch will delete these files if they are older than 30 days. To change this, add the following to your configuration file: <pre><code>delete_stale_files_after: &lt;number of days&gt;\n</code></pre></p>"},{"location":"contributing/","title":"Contributing to SiteWatch","text":"<p>SiteWatch is an open-source tool, and we welcome contributions from the community. This document outlines the process for contributing to SiteWatch.</p>"},{"location":"contributing/#the-sitewatch-structure","title":"The SiteWatch Structure","text":"<p>SiteWatch is written in Python for readability and portability. The code structrue is as follows</p>"},{"location":"contributing/#the-sitewatch-file","title":"The <code>sitewatch</code> file","text":"<p>This file is the entry point for the application. It contains the logic for greeting and parsing command line arguments, verifying the config and CSV files, and looping over each test and initiating it. It also contains the logic for writing the results to the output file and initiating the email process. This file is the only file that should be run directly, and it has been setup to be an executable file. As this file is relatively short (all other logic is packed into other files), it shouldn't need to be modified often.</p>"},{"location":"contributing/#the-utils-directory","title":"The <code>utils</code> Directory","text":"<p>This directory contains various useful files with many utility functions. The <code>config_utils.py</code> module contains the logic for reading the config file and verifying that it is valid. The <code>csv_utils.py</code> module contains the logic for reading the CSV file and verifying that it is valid. The <code>email_utils.py</code> module contains the logic for sending emails. The documentation for the functions contained in these files is contained in the docstrings of the functions themselves, which makes it easy to use if you are editing the code in an IDE.</p>"},{"location":"contributing/#the-test_suites-directory","title":"The <code>test_suites</code> Directory","text":"<p>This directory contains logic for running tests. The main file to make note of here is <code>test_controller.py</code>, as this is the module which contains the <code>TestController</code> class, which can run tests given the name of the test and a CSV row. </p>"},{"location":"contributing/#the-pages-directory","title":"The <code>pages</code> Directory","text":"<p>This directory contains logic for interacting with web pages. The main file to make note of here is <code>page.py</code>, as this is the module which contains the <code>BasePage</code> class, which is the base class for all pages (and it contains several useful functions applicable to all pages). When a page module is added to this directory, it should contain logic that is specific to that page. For example, the <code>collection_page.py</code> contains several functions that interact with viewers, which are specific to the collection page.</p>"},{"location":"contributing/#adding-a-new-test","title":"Adding a New Test","text":"<p>To better illustrate the process of adding a new test, we will walk through the process of adding a test. Specifically, let's test the number of results when accessing a search page or a collections page. For example, on this page, there are 3 results, and on this page, there are 23591 results. We want to design a test that compares the number of results on the page to the expected number of results that the user specifies in the CSV file, and fails if the numbers do not match.</p> <p>There are several steps involved in adding a new test:</p> <ul> <li>Find an appropriate element to test</li> <li>Decide if you need a new page module and create one if necessary (must inherit from <code>BasePage</code>)</li> <li>In the page class you are testing, create logic to interact with the element you are testing</li> <li>Decide if you need a new test suite and create one if necessary (must inherit from <code>Test</code>)</li> <li>In the test suite, create a new test function with assert statements</li> <li>Add the test to the <code>test_controller.py</code> file</li> <li>Whitelist the test in the CSV checks</li> <li>Add required test input checks to the CSV checks</li> </ul>"},{"location":"contributing/#find-an-appropriate-element-to-test","title":"Find an Appropriate Element to Test","text":"<p>Let's consider the first page above. Here is a screenshot of the page</p> <p></p> <p>We want to confirm that there are 3 results on the page. First, ask yourself: \"How would I, as a human, confirm that there are 3 results on the page?\" You would probably look for something that says the total number of results. Indeed, the element we are looking for is right in front of us!</p> <p></p> <p>If we can get to the \"3\" part, then we'll know how many results we have.</p> <p>We now use the Chrome DevTools to inspect this element and find something (like a class name, ID, XPath, or CSS selector) that we can use to identify this element, so that we can search for it in our code. To do this, right click on the element and click \"Inspect\". </p> <p></p> <p>This will open the DevTools and highlight the element we are looking for in blue.</p> <p> </p> <p>This is indeed the element we are looking for since it clearly says \"Displaying 1 - 3 of 3\". Now, we need to find something that we can use to identify this element. Typically, the following order for choosing what to identify the element by should be selected as follows:</p> <ul> <li>ID</li> <li>Class Name (if it is unique)</li> <li>XPath</li> <li>CSS Selector We can see that the element has a class name of <code>pager__summary</code>, and this is unique to this element, so we will use this to identify the element. That's all we need from the webpage! We can close the entire browser if we want now.</li> </ul>"},{"location":"contributing/#decide-if-you-need-a-new-page-module-and-create-one-if-necessary","title":"Decide if You Need a New Page Module and Create One if Necessary","text":"<p>At the time of writing, the only module in the <code>page</code> directory is <code>page.py</code>, and as that module contains logic for interacting with a general page, and this test pertains only to search and collections pages, we'll make a new module <code>collections_or_advanced_search_page.py</code>. </p>"},{"location":"contributing/#in-the-page-class-you-are-testing-create-logic-to-interact-with-the-element-you-are-testing","title":"In the Page Class You Are Testing, Create Logic to Interact with the Element You Are Testing","text":"<p>We'll cut to the chase and show the code we wrote straight away, and then explain it.</p> <pre><code>from pages.page import BasePage\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.webdriver.common.by import By\nfrom typing import Optional\nclass CollectionsOrAdvancedSearchPage(BasePage):\ndef get_collection_count(self) -&gt; Optional[int]:\n\"\"\"Return the number of collections on the collections page.\"\"\"\nself.driver.get(self.url) # (1)\ntry:\npager_summary = self.driver.find_element(By.CLASS_NAME, \"pager__summary\")\nreturn int(pager_summary.text.split(\" \")[-1])\nexcept NoSuchElementException:\nreturn None\n</code></pre> <ol> <li>How does the function know which page to navigate to? The <code>BasePage</code> class has a constructor that takes in a URL (along with other parameters), and the <code>get_collection_count</code> function uses this URL to navigate to the page. See the <code>BasePage</code> class for more details.</li> </ol> <p>As you can see, we have a class called <code>CollectionsOrAdvancedSearchPage</code> that inherits from <code>BasePage</code>. We have a function called <code>get_collection_count</code> that returns the number of collections on the page. This function does the following:</p> <ul> <li>Navigates to the page</li> <li>Finds the element we are looking for</li> <li>Gets the text of the element</li> <li>Splits the text by spaces</li> <li>Gets the last element of the list (which is the number of collections)</li> <li>Converts the number of collections to an integer</li> <li>Returns the number of collections</li> </ul>"},{"location":"contributing/#decide-if-you-need-a-new-test-suite-and-create-one-if-necessary","title":"Decide if You Need a New Test Suite and Create One if Necessary","text":"<p>At the time of writing, the only module in the <code>test_suites</code> directory is <code>test.py</code>, and as that module contains only the abstract <code>Test</code> class, we'll make a new module <code>collection_count_test.py</code>.</p>"},{"location":"contributing/#in-the-test-suite-create-a-new-test-function-with-assert-statements","title":"In the Test Suite, Create a New Test Function with Assert Statements","text":"<p>We'll cut to the chase and show the code we wrote straight away, and then explain it.</p> <pre><code>from pages.collections_or_advanced_search_page import CollectionsOrAdvancedSearchPage\nfrom test_suites.test import Test\nfrom selenium.common.exceptions import NoSuchElementException\nclass CollectionCountTest(Test):\n\"\"\" A test to check that the number of collections on the collections page is correct. \"\"\"    \ndef run(self, url: str, expected_value: str) -&gt; None:\n\"\"\" Run the test on the page at &lt;url&gt; and compare the result to &lt;expected_value&gt;.\"\"\"\ntry:\nexpected_value = int(expected_value)\nexcept ValueError:\nraise ValueError(f\"Expected value must be an integer, but got {expected_value}.\")\ncollection_page = CollectionsOrAdvancedSearchPage(self.driver, url)\ntry:\nactual_value = int(collection_page.get_collection_count())\nexcept NoSuchElementException:\nraise NoSuchElementException(f\"Could not find the collections count on {url}.\")\nassert actual_value == expected_value, f\"Expected {expected_value}, but got {actual_value}.\"\n</code></pre> <p>As you can see, we have a class called <code>CollectionCountTest</code> that inherits from <code>Test</code>. We have a function called <code>run</code> that runs the test. This function does the following:</p> <ul> <li>Tries to convert the expected value to an integer</li> <li>Creates a <code>CollectionsOrAdvancedSearchPage</code> object</li> <li>Tries to get the number of collections on the page</li> <li>Asserts that the actual value is equal to the expected value</li> </ul>"},{"location":"contributing/#add-the-test-to-the-test_controllerpy-file","title":"Add the Test to the <code>test_controller.py</code> File","text":"<p>This itself contains multiple steps:</p> <ul> <li>Import the test suite</li> <li>Make an attribute for the <code>TestController</code> that points to an instance of your new test suite</li> <li>Make a function that runs your test suite</li> <li>Add your test to the <code>TestController</code>'s <code>run_test</code> function</li> </ul> <p>We'll first import test suite:  <pre><code>from test_suites.collection_count_test import CollectionCountTest\n</code></pre></p> <p>Then, we'll add the following line to the bottom of the initializer of the <code>TestController</code> class:</p> <p><pre><code>self.collection_count_test = CollectionCountTest(self.driver)\n</code></pre> So now the initializer looks like this:</p> <pre><code>class TestController():\ndef __init__(self):\noptions = webdriver.ChromeOptions()\noptions.add_argument(\"--headless\")\nself.driver.implicitly_wait(20)\nself.driver = webdriver.Chrome(options=options)\nself.collection_count_test = CollectionCountTest(self.driver)\n</code></pre> <p>Then, we'll make a function that runs the test suite: <pre><code>def run_collection_count_test(self, csv_row: dict, csv_row_number: int) -&gt; bool:\n\"\"\" Runs a Collection Count Test. \"\"\"\ntry:\nself.collection_count_test.run(csv_row[\"url\"], csv_row[\"test_input\"])\nexcept ValueError:\nprint(Fore.RED, f\"Invalid test input on row {csv_row_number + 1}. Please see log for more details.\")\nlogging.error(f\"Invalid test input on row {csv_row_number + 1}. The test input must be an integer.\")\nreturn False\nexcept AssertionError as e:\n# Get the assertion error message\nerror_message = str(e)\nprint(Fore.RED, f\"Collection Count Test failed on row {csv_row_number + 1}. Please see log for more details.\")\nlogging.error(f\"Collection Count Test failed on row {csv_row_number + 1}. The expected number of collections was not found. {error_message}\")\nreturn False\nexcept Exception as e:\nprint(Fore.RED, f\"Collection Count Test failed on row {csv_row_number + 1}. Please see log for more details.\")\nlogging.error(f\"Collection Count Test failed on row {csv_row_number + 1}. {e}\")\nreturn False\nelse:\nprint(Fore.GREEN, f\"Collection Count Test passed on row {csv_row_number + 1}.\")\nlogging.info(f\"Collection Count Test passed on row {csv_row_number + 1}.\")\nreturn True\n</code></pre> We must make sure that under no circumstances does the test suite crash, so we have a try-except block that catches all exceptions. If the test passes, we print a green message to the console and log a message to the log file. If the test fails, we print a red message to the console and log a message to the log file.</p> <p>Finally, we'll add the following to the <code>run_test</code> function's <code>test_methods</code> dictionary: <pre><code>test_methods = {\n...\n'collection_count_test': self.run_collection_count_test\n}\n</code></pre></p>"},{"location":"contributing/#whitelist-the-test-in-the-csv-checks","title":"Whitelist the Test in the CSV Checks","text":"<p>Go to <code>csv_utils.py</code> and find the <code>check_data</code> function. There will be a variable <code>supported_test_types</code> that contains a list of all the test types that are supported. Add your test type to this list. We'll just add a new element `collection_count_test' to the list.</p>"},{"location":"contributing/#add-required-test-input-checks-to-the-csv-checks","title":"Add Required Test Input Checks to the CSV Checks","text":"<p>As this test requires the user to input the expected number of results in the CSV, we must add it to the CSV checks. Go to <code>csv_utils.py</code> and find the <code>check_data</code> function. There will be a variable <code>test_types_with_input</code> that contains a dictionary of all the test types that require test input. Add your test type to this dictionary. We'll just add a new key-value pair <code>collection_count_test''</code> to the dictionary. <pre><code>test_types_with_input = {\n...\n'collection_count_test'\n}\n</code></pre> If your test requires more extensive checks (like if the test input must contain two elements separated by a \"|\"), you can add those checks to the <code>check_data</code> function.</p> <p>And that's it! You've added a new test to the test suite. Now, you can run the test suite and see if it works. If it doesn't, you can debug it and fix it.</p> <p>And that's it! You've added a new test to the test suite. Now, you can run the test suite and see if it works. If it doesn't, you can debug it and fix it.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.7 or higher</li> <li>The latest Google Chrome and Chromedriver installed to the PATH (see Installing Chrome and Chromedriver on Linux or visit the Google Chrome and Chromedriver websites for instructions on installing these on other operating systems)</li> <li>The following Python libraries:<ul> <li>ruamel.yaml</li> <li>Requests</li> <li>openpyxl</li> <li>rich</li> <li>selenium</li> <li>colorama</li> <li>If you want to have these libraries automatically installed, you will need Python's setuptools</li> </ul> </li> <li>Highly recommended: A Linux-based operating system (e.g. Ubuntu, Debian, Fedora, etc.)</li> </ul>"},{"location":"installation/#installing-sitewatch","title":"Installing SiteWatch","text":"<p>Installation involves three steps:</p> <ol> <li>cloning the SiteWatch Github repo</li> <li>running <code>setup.py</code> to install the required Python libraries (listed above)</li> </ol>"},{"location":"installation/#step-1-cloning-the-sitewatch-repo","title":"Step 1: Cloning the SiteWatch Repo","text":"<p>In a terminal, run:</p> <p><code>git clone https://github.com/digitalutsc/site_watch.git</code></p> <p>This will create a directory named <code>site_watch</code> where you will run the <code>./sitewatch</code> command. Change into this directory: <pre><code>cd site_watch\n</code></pre> and run the following command to make the <code>./sitewatch</code> and <code>./setupwizard</code> commands executable: <pre><code>chmod +x sitewatch setupwizard\n</code></pre></p>"},{"location":"installation/#step-2-running-setuppy-to-install-the-required-python-libraries","title":"Step 2: Running setup.py to Install the Required Python Libraries","text":"<p>For most users, the preferred place to install Python libraries is in the user directory. To do this, change into the \"site_watch\" directory created by cloning the repo, and run the following command:</p> <p><code>python3 setup.py install --user</code></p> <p>A less common mehtod is to install the required Python libraries into your computer's central Python environment. To do this, omit the <code>--user</code> (note: you must have administrator privileges on the computer to do this):</p> <p><code>sudo python3 setup.py install</code></p>"},{"location":"installation/#updating-sitewatch","title":"Updating SiteWatch","text":"<p>Since SiteWatch is under development, you will want to update it often. To do this, within the <code>site_watch</code> directory, run the following <code>git</code> command:</p> <p><code>git pull origin main</code></p> <p>After you pull in the latest changes using <code>git</code>, it's a good idea to rerun the setup tools in case new Python libraries have been added since you last ran the setup tools (same command as above):</p> <p><code>python3 setup.py install --user</code></p> <p>or if you originally installed the required Python libraries centrally, without the <code>--user</code> option (again, you will need administrator privileges on the machine):</p> <p><code>sudo python3 setup.py install</code></p>"},{"location":"installation/#installing-google-chrome-and-chromedriver-on-linux","title":"Installing Google Chrome and Chromedriver on Linux","text":"<p>SiteWatch uses Google Chrome and Chromedriver to run tests. You will need to install both of these on the machine where you will be running SiteWatch. </p> <p>To install Google Chrome, run the following in a terminal:</p> <pre><code>sudo apt-get update -y\nsudo apt install wget -y\nsudo wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\nsudo dpkg -i google-chrome-stable_current_amd64.deb\nsudo apt -f install\nsudo rm google-chrome-stable_current_amd64.deb\n</code></pre> <p>To install Chromedriver, run the following in a terminal:</p> <pre><code>sudo apt-get update -y\nsudo apt-get install unzip -y\nsudo apt-get install curl -y\nlatest_version=$(curl -sL https://chromedriver.chromium.org/downloads | grep -oP 'ChromeDriver \\K([0-9]+.[0-9]+.[0-9]+.[0-9]+)' | head -n 1)\ndownload_url=\"https://chromedriver.storage.googleapis.com/$latest_version/chromedriver_linux64.zip\"\ntemp_dir=$(mktemp -d)\nsudo curl -sL \"$download_url\" -o \"$temp_dir/chromedriver.zip\"\nsudo unzip \"$temp_dir/chromedriver.zip\" -d \"$temp_dir\"\nsudo mv \"$temp_dir/chromedriver\" /usr/local/bin/\nsudo rm -rf \"$temp_dir\"\n</code></pre>"},{"location":"output/","title":"Output In SiteWatch","text":"<p>Sitewatch has the capability to convey the results of its tests in a variety of ways:</p> <ul> <li>To the console (stdout)</li> <li>To a log file</li> <li>To a CSV file</li> <li>To email addresses</li> </ul> <p>The first three of these are by default enabled, and the last is up to the user to enable. The following sections describe each of these in detail.</p>"},{"location":"output/#console-output","title":"Console Output","text":"<p>SiteWatch provides a nice, human-readable summary of its results to the console. Here is an example of what that looks like:</p> <p></p>"},{"location":"output/#log-file","title":"Log File","text":"<p>SiteWatch writes a log file to the <code>logs</code> directory. The name of the file is time-stamped to be unique. The name of the log file is the same as the name of the output CSV file (see below), but with a <code>.log</code> extension. For example, if the output CSV file is named <code>site_watch-2023-06-14-13-02-23.csv</code>, the log file will be named <code>site_watch-2023-06-14-13-02-23.log</code>. The log file serves to report important information about the results of tests and, if they failed, why they failed. Here is an example of what the log file for the above console output might look like: <pre><code>INFO:root:SiteWatch has started.\nINFO:utils.csv_utils:CSV file is valid.\nINFO:test_suites.test_controller:Site Availability Test passed on row 2.\nINFO:test_suites.test_controller:Collection Count Test passed on row 3.\nINFO:test_suites.test_controller:OpenSeaDragon Load Test passed on row 4.\nINFO:test_suites.test_controller:Mirador Load Test passed on row 5.\nINFO:test_suites.test_controller:AblePlayer Transcript Load Test passed on row 6.\nINFO:test_suites.test_controller:Mirador Load Test passed on row 7.\nERROR:test_suites.test_controller:Mirador Page Count Test failed on row 8. Mirador viewer does not have the expected number of thumbnails. Expected 1032, got 1034.\nINFO:root:All tests have finished running.\nINFO:root:Results have been written to output_csvs/site_watch-2023-06-14-13-02-23.csv\n</code></pre></p>"},{"location":"output/#output-csv","title":"Output CSV","text":"<p>SiteWatch writes a CSV file to the <code>output_csvs</code> directory. The name of the file is time-stamped to be unique. The name of the output CSV file is the same as the name of the log file (see above), but with a <code>.csv</code> extension. For example, if the log file is named <code>site_watch-2023-06-14-13-02-23.log</code>, the output CSV file will be named <code>site_watch-2023-06-14-13-02-23.csv</code>. The output CSV is a copy of the input test data file, but with two columns appended:</p> <ul> <li><code>test_result</code>: The result of the test. This will be either <code>Passed</code> or <code>Failed</code>.</li> <li><code>total_time</code>: The total time it took to run the test, in seconds. For example, if the original input data file looked like <pre><code>url,test_type,input_data\nhttps://memory.digital.utsc.utoronto.ca/collection/33463,collection_count_test,23591\n</code></pre> the output CSV file would look like <pre><code>url,test_type,input_data,test_result,total_time\nhttps://memory.digital.utsc.utoronto.ca/collection/33463,collection_count_test,23591,Passed,0.5\n</code></pre></li> </ul>"},{"location":"output/#email","title":"Email","text":"<p>SiteWatch has the capability to send an email to one or more email addresses when it finishes running, only in the event of error or failed test case. This is useful for notifying people when a test has failed. The email sent will contain a copy of the log and output CSV files. To enable this feature, see Email Settings.</p>"},{"location":"writing-test-data/","title":"The Test File","text":"<p>SiteWatch gets its second input from a CSV-like file containing the tests that SiteWatch is to run. This file specifies the URLs to test, the type of test to be run, expected values, and other information. For example, a test file might look like this: <pre><code>url,test_type,input_data\nhttps://memory.digital.utsc.utoronto.ca/collection/33463,collection_count_test,23591\nhttps://memory.digital.utsc.utoronto.ca/,site availability test,\n</code></pre></p>"},{"location":"writing-test-data/#required-test-file-columns","title":"Required Test File Columns","text":"<p>While the test file can contain any number of columns (for example, a description column), it must contain the following columns:</p> <ul> <li><code>url</code>: The URL to test.</li> <li><code>test_type</code>: The type of test to run. See Test Types below for a list of valid test types.</li> <li><code>input_data</code>: For some test types, this column is required, and is used to provide more information about the tests. The format of this column depends on the test type. See Test Types below for more information.</li> </ul>"},{"location":"writing-test-data/#test-types","title":"Test Types","text":"<p>SiteWatch supports the following test types:</p>"},{"location":"writing-test-data/#site_availability_test","title":"<code>site_availability_test</code>:","text":"<p>This test type checks to see if the site is available. It does not require any input data. It performs a <code>GET</code> request to the URL, and checks to see if the response code is indicative of a successful request (i.e., a 2xx response code). Then, it double-checks by scanning the response body for a string that indicates the site is not available. Then, it triple-checks by actually visiting the site in a browser and checking to see if it is available. If any of these checks fail, the test fails. Here is a sample test row for this test type: <pre><code>url,test_type,input_data\nhttps://memory.digital.utsc.utoronto.ca/,site_availability_test,\n</code></pre> As shown above, the <code>input_data</code> column is empty, as this test type does not require any input data.</p>"},{"location":"writing-test-data/#collection_count_test","title":"<code>collection_count_test</code>:","text":"<p>This test type checks to see if the number of items in a collection matches the expected number of items. It requires the expected number of items to be passed as input. The test will then visit the collection page in a browser and check to see if the number of items matches the expected number of items. Here is a sample test row for this test type that checks to see if the collection has 23,591 items. <pre><code>url,test_type,input_data\nhttps://memory.digital.utsc.utoronto.ca/collection/33463,collection_count_test,23591\n</code></pre> By default, the test will check to see if the collection count matches the expected count within 20 seconds.</p>"},{"location":"writing-test-data/#openseadragon_load_test","title":"<code>openseadragon_load_test</code>:","text":"<p>This test type checks to see if the OpenSeadragon viewer loads on an item page. It does not require any input data. The test will then visit the item page in a browser and check to see if the OpenSeadragon viewer loads. Here is a sample test row for this test type: <pre><code>url,test_type,input_data\nhttps://memory.digital.utsc.utoronto.ca/61220/utsc16185,openseadragon_load_test,\n</code></pre> By default, the test will check to see if the OpenSeadragon viewer loads within 20 seconds.</p>"},{"location":"writing-test-data/#mirador_viewer_load_test","title":"<code>mirador_viewer_load_test</code>:","text":"<p>This test type checks to see if the Mirador viewer loads on an item page. It does not require any input data. The test will then visit the item page in a browser and check to see if the Mirador viewer loads. Here is a sample test row for this test type: <pre><code>url,test_type,input_data\nhttps://memory.digital.utsc.utoronto.ca/61220/utsc11802,mirador_viewer_load_test,\n</code></pre> By default, the test will check to see if the Mirador viewer loads within 40 seconds.</p>"},{"location":"writing-test-data/#mirador_page_count_test","title":"<code>mirador_page_count_test</code>:","text":"<p>This test type checks to see if the number of pages in a Mirador viewer matches the expected number of pages. It requires the expected number of pages to be passed as input. The test will then visit the item page in a browser and check to see if the number of pages matches the expected number of pages. Here is a sample test row for this test type that checks to see if the Mirador viewer has 1034 pages. <pre><code>url,test_type,input_data\nhttps://tamil.digital.utsc.utoronto.ca/61220/utsc34439,mirador_page_count_test,1034\n</code></pre> By default, the test will check to see if the Mirador viewer has the expected number of pages within 40 seconds.</p>"},{"location":"writing-test-data/#ableplayer_load_test","title":"<code>ableplayer_load_test</code>:","text":"<p>This test type checks to see if the AblePlayer viewer loads on an item page. It does not require any input data. The test will then visit the item page in a browser and check to see if the AblePlayer viewer loads. Here is a sample test row for this test type: <pre><code>url,test_type,input_data\nhttps://tamil.digital.utsc.utoronto.ca/61220/utsc34400,ableplayer_load_test,\n</code></pre> By default, the test will check to see if the AblePlayer viewer loads within 20 seconds.</p>"},{"location":"writing-test-data/#ableplayer_transcript_load_test","title":"<code>ableplayer_transcript_load_test</code>:","text":"<p>This test type checks to see if the AblePlayer transcript loads on an item page. It does not require any input data. The test will then visit the item page in a browser and check to see if the AblePlayer transcript loads. Here is a sample test row for this test type: <pre><code>url,test_type,input_data\nhttps://tamil.digital.utsc.utoronto.ca/61220/utsc34400,ableplayer_transcript_load_test,\n</code></pre> By default, the test will check to see if the AblePlayer transcript loads within 20 seconds.</p>"},{"location":"writing-test-data/#element_present_test","title":"<code>element_present_test</code>:","text":"<p>This test takes allows the checking of the presence of any element on a page. It requires two inputs: the method to search by, and the search term. The method can be one of the following:</p> <ul> <li><code>id</code>: Search by element ID.</li> <li><code>class</code>: Search by element class.</li> <li><code>xpath</code>: Search by XPath.</li> <li><code>css</code>: Search by CSS selector.</li> </ul> <p>The search term is the value to search for. The two inputs must be provided in the CSV file \"test_input\" column and be separated by a subdelimeter \"|\" (pipe). Here is a sample test row for this test type: <pre><code>url,test_type,test_input\nhttps://tamil.digital.utsc.utoronto.ca/collection/2855,element_present_test,xpath|/html/body/div/div[2]/div/div[2]/div/div/div/div/div/div[2]/div/aside/div[3]/h2\n</code></pre></p>"},{"location":"writing-test-data/#invalid_links_test","title":"<code>invalid_links_test</code>:","text":"<p>This test (non-recursively) tests all links present on the given URL to see if they are broken. It does not require any input data. If the test fails, it will provide a list of broken links in the <code>.log</code> file. Here is a sample test row for this test type: <pre><code>url,test_type,input_data\nhttps://digital.utsc.utoronto.ca/basic-page/systems-and-software,invalid_links_test,\n</code></pre></p> <p>Warning</p> <p>This test can take a long time to run, depending on the number of links on the page, since it opens each link in a browser and waits for the page to load. As the implementation of this test is multi-threaded, it is recommended to run this test on a machine with a higher number of threads in its processor.</p> <p>Note</p> <p>You may get a warning in the <code>.log</code> file similar to the following:</p> <pre><code>WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: 127.0.0.1. Connection pool size: 1\n</code></pre> <p>This is normal, and you can safely discard it.</p>"},{"location":"writing-test-data/#permalink_redirect_test","title":"<code>permalink_redirect_test</code>:","text":"<p>This test checks to see if the ARK resolver correctly redirects the permalink on a collection page to the correct location. It requires the expected redirect URL to be passed as input. The test will then visit the collection page in a browser and check to see if the permalink redirects to the expected URL. Here is a sample test row for this test type: <pre><code>url,test_type,input_data\nhttps://ark.digital.utsc.utoronto.ca/ark:/61220/utsc11324,permalink_redirect_test,https://tamil.digital.utsc.utoronto.ca/61220/utsc11324\n</code></pre></p>"},{"location":"writing-test-data/#rest_oai_pmh_xml_validity_test","title":"<code>rest_oai_pmh_xml_validity_test</code>:","text":"<p>This test checks if the XML on the OAI-PMH endpoint is valid. It does not require any input data. Here is a sample test row for this test type: <pre><code>url,test_type,input_data\nhttps://memory.digital.utsc.utoronto.ca/oai/request?identifier=oai%3Amemory.digital.utsc.utoronto.ca%3Anode-10262&amp;metadataPrefix=mods&amp;verb=GetRecord,permalink_redirect_test,\n</code></pre></p>"}]}